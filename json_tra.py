# scripts/build_styled_json.py
import re, json, time, pathlib, pandas as pd, torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain.document_loaders import CSVLoader, PyPDFLoader
# 1. ì—‘ì…€ ë¡œë“œ (Hì—´ = index 7)

pdf_loader = PyPDFLoader(file_path='EventGameDocument.pdf')

# ë¬¸ì„œ ë¡œë“œ
pdf_documents = pdf_loader.load()
cleaned_pdf_content = pdf_documents[0].page_content.replace('\n', ' ')

# ë¡œë“œëœ ë¬¸ì„œ í™•ì¸

# 2. ëª¨ë¸ ë¡œë“œ (Chat ë²„ì „ í•„ìˆ˜)
model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id, device_map="auto", torch_dtype=torch.bfloat16
)

# 3. Qwen í…œí”Œë¦¿ í•¨ìˆ˜
def qwen_chat(system, user):
    messages = [
        {"role": "system", "content": system},
        {"role": "user",   "content": user},
        {"role": "assistant", "content": ""}
    ]
    tpl = tok.apply_chat_template(messages, tokenize=False)
    return tok(tpl, return_tensors="pt").to(model.device)

SYS_PROMPT = (
   """ 
You are a document parser. 
Given raw text tagged by page boundaries, split it into sentences and output a JSON array.
Each item must have:
- title: text title
- page: integer page number
- text: the sentence text

Only output valid JSON; do not include any additional explanation.
Example (do NOT wrap in ```json):
{"THá»‚ Lá»† CHUNG":{"title": "Äiá»u kiá»‡n tham gia" ,"text":"Táº¥t cáº£ ngÆ°á»i chÆ¡i cáº§n Ä‘Äƒng kÃ½ tÃ i khoáº£n báº±ng email hoáº·c sá»‘ Ä‘iá»‡n thoáº¡i"...}....}
"""
)

json_re = re.compile(r"\{[\s\S]*?\}", re.S)

def convert_styles(sentence: str):
    usr = f'Input: "{cleaned_pdf_content}"\nReturn the JSON.'
    inp = qwen_chat(SYS_PROMPT, usr)
    prompt_len = inp["input_ids"].shape[-1]

    out_ids = model.generate(
        input_ids      = inp["input_ids"],
        attention_mask = inp["attention_mask"],
        max_new_tokens = 128,
        temperature    = 0.7
    )

    text = tok.decode(out_ids[0][prompt_len:], skip_special_tokens=True)
    match = json_re.search(text)
    print(text)
    print(match)
    if not match:
        print("âš ï¸  JSON not found:", sentence[:40])
        return None
    try:
        return json.loads(match.group())
    except json.JSONDecodeError:
        print("âš ï¸  Parse error:", sentence[:40])
        return None

# # 4. ë©”ì¸ ë£¨í”„
# results = []
# for i, sent in enumerate(cleaned_pdf_content, 1):
#     print(f"ğŸ”„ ({i}/{len(cleaned_pdf_content)}) {sent[:40]}...")
#     var = convert_styles(sent)
#     if var:
#         results.append({
#             "{sent}":{
#               {
#                 "title": {"q": var.get("Black English", "")},
#                 "White English": {"q": var.get("White English", "")},
#                 "British English": {"q": var.get("British English", "")}
#             }}
#         })
#     time.sleep(0.4)
# out_path = pathlib.Path("styled_questions.json")

# # 5. ì €ì¥
# if out_path.exists():
#     try:
#         existing = json.loads(out_path.read_text(encoding="utf-8"))
#         if isinstance(existing, list):
#             combined = existing + results
#         else:
#             # ê¸°ì¡´ íŒŒì¼ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë°±ì—…í•˜ê³  ìƒˆë¡œ ì‹œì‘
#             backup = out_path.with_suffix(".backup.json")
#             out_path.rename(backup)
#             print(f"âš ï¸ ê¸°ì¡´ JSONì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ë°±ì—…: {backup.name}")
#             combined = results
#     except json.JSONDecodeError:
#         print("âš ï¸ ê¸°ì¡´ JSON íŒŒì‹± ì‹¤íŒ¨. ë®ì–´ì“°ê¸°í•˜ì§€ ì•Šê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
#         combined = results
# else:
#     combined = results

# # ë®ì–´ì“°ê¸° ì €ì¥
# out_path.write_text(
#     json.dumps(combined, ensure_ascii=False, indent=2),
#     encoding="utf-8"
# )
# print(f"âœ… styled_questions.json ì—…ë°ì´íŠ¸ ì™„ë£Œ! ì´ í•­ëª©: {len(combined)} ë¬¸ì¥")
